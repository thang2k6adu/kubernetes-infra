# üöÄ H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI K3S CLUSTER (MASTER + WORKER)

## B∆Ø·ªöC 2: SET IP Tƒ®NH + DISABLE CLOUD-INIT (MASTER)

Disable cloud-init network:
```bash
sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
```

N·ªôi dung:
```yaml
network: {config: disabled}
```

X√≥a netplan c≈©:
```bash
sudo rm -f /etc/netplan/50-cloud-init.yaml
```

T·∫°o netplan m·ªõi:
```bash
sudo nano /etc/netplan/01-static.yaml
```

N·ªôi dung:
```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    ens33:
      dhcp4: no
      addresses:
        - 192.168.0.50/24
      gateway4: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 1.1.1.1
```

Apply:
```bash
sudo netplan apply
```

## B∆Ø·ªöC 1: ƒê·ªîI HOSTNAME (TR√äN NODE MASTER)

Nh·ªõ d√πng `ip a` ƒë·ªÉ check **IP / mask / gateway** v√† thay cho ƒë√∫ng tr∆∞·ªõc khi l√†m b·∫•t c·ª© ƒëi·ªÅu g√¨.
```bash
sudo hostnamectl set-hostname k3s-master
sudo nano /etc/hosts
```

V√≠ d·ª• n·ªôi dung:
```txt
127.0.0.1 localhost
192.168.0.50 k3s-master
```

Reboot:
```bash
sudo reboot
```

Check IP:
```bash
ip a
```

## B∆Ø·ªöC 3: SCAN IP C√ÅC SERVER WORKER (TR√äN MASTER)

C√†i `nmap`:
```bash
sudo apt install nmap -y
```

Auto generate inventory file

‚ö†Ô∏è Nh·ªõ s·ª≠a subnet + port SSH cho ƒë√∫ng m√¥i tr∆∞·ªùng. Sau n√†y th√™m server th√¨ nh·ªõ ch·∫°y l·∫°i c√°i n√†y l√† oke.
```bash
SUBNET=192.168.0.0/24
PORT=8022
USER="thang2k6adu"
START_IP=51
MASTER_IP=$(hostname -I | awk '{print $1}')
BASE_IP=$(echo $SUBNET | cut -d'/' -f1 | awk -F. '{print $1"."$2"."$3}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "^$MASTER_IP$" \
| awk -v USER="$USER" -v PORT="$PORT" -v BASE="$BASE_IP" -v START="$START_IP" \
'{print $0" ansible_user="USER" ansible_port="PORT" worker_ip="BASE"."START+NR-1}' \
>> hosts.ini

cd ~/
```

Check file inventory:
```bash
cat ~/k3s-inventory/hosts.ini
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```ini
[master]
192.168.0.50 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.50

[workers]
192.168.0.108 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.51
192.168.0.109 ansible_user=thang2k6adu ansible_port=8022 worker_ip=192.168.0.52
```

## B∆Ø·ªöC 4: C√ÄI K3S CONTROL PLANE (MASTER)

ƒê·∫∑t t√™n node l√† `k3s-master`:
```bash
curl -sfL https://get.k3s.io | sh -s - \
  --write-kubeconfig-mode 644 \
  --node-name k3s-master
```

Check:
```bash
kubectl get nodes
```

## B∆Ø·ªöC 5: M·ªû FIREWALL (UFW)

### Master:
```bash
sudo ufw allow 6443/tcp   # worker k·∫øt n·ªëi v·ªÅ master
sudo ufw allow 8472/udp   # pod giao ti·∫øp
sudo ufw allow 10250/tcp  # l·∫•y log pod
```

### Worker (b·∫±ng Ansible):
```bash
sudo ufw allow 8472/udp
sudo ufw allow 10250/tcp
```

## C√ÄI ANSIBLE TR√äN MASTER
```bash
sudo apt update
sudo apt install ansible -y
```

L∆∞u √Ω ph·∫£i l·∫Øp ssh v√†o master node tr∆∞·ªõc khi ssh

L·∫•y ssh private key ƒë√£ b·ªè v√†o c√°c node (l√∫c setup) r·ªìi b·ªè l√™n master
·ªü ƒë√¢y ch·ªâ c√≥ h∆∞·ªõng d·∫´n window
scp -P 8022 $env:USERPROFILE\.ssh\id_ed25519 thang2k6adu@192.168.0.50
:/home/thang2k6adu/.ssh/id_ed25519

l·∫•y public key b·ªè v√†o
scp -P 8022 $env:USERPROFILE\.ssh\id_ed25519.pub thang2k6adu@192.168.0.50:/home/thang2k6adu/.ssh/id_ed25519.pub

ph√¢n quy·ªÅn
chmod 700 ~/.ssh
chmod 600 ~/.ssh/id_ed25519
```

Test k·∫øt n·ªëi:
```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m ping
```

## SET SUDO KH√îNG PASSWORD (CHO WORKER)

T·∫°o file:
```bash
nano ~/k3s-inventory/setup-sudo.yml
```
```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow thang2k6adu sudo without password
      copy:
        dest: /etc/sudoers.d/thang2k6adu
        content: |
          thang2k6adu ALL=(ALL) NOPASSWD:ALL
        owner: root
        group: root
        mode: '0440'
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/setup-sudo.yml -K
```

t·∫°o playbook gen card m·∫°ng

nano ~/k3s-inventory/gen_iface.yml

- hosts: master,workers
  gather_facts: yes
  vars:
    inventory_file: "{{ playbook_dir }}/hosts.ini"

  tasks:
    - name: Update inventory with iface
      delegate_to: localhost
      lineinfile:
        path: "{{ inventory_file }}"
        regexp: "^{{ inventory_hostname }}\\s"
        line: "{{ inventory_hostname }} ansible_user={{ ansible_user }} ansible_port={{ ansible_port }} worker_ip={{ hostvars[inventory_hostname].worker_ip }} iface={{ ansible_default_ipv4.interface }}"

check
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/gen_iface.yml -K

check
cat ~/k3s-inventory/hosts.ini

## SET IP Tƒ®NH CHO WORKER
```bash
nano ~/k3s-inventory/set-static-ip.yml
```
```yaml
- hosts: workers
  become: yes
  vars:
    dns:
      - 8.8.8.8
      - 1.1.1.1

  tasks:
    - name: Disable cloud-init network
      copy:
        dest: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
        content: |
          network: {config: disabled}

    - name: Remove old netplan config
      file:
        path: /etc/netplan/50-cloud-init.yaml
        state: absent

    - name: Configure static IP
      template:
        src: static.yaml.j2
        dest: /etc/netplan/01-static.yaml
        mode: '0644'

    - name: Apply netplan
      command: netplan apply
      async: 10
      poll: 0
```

```bash
nano ~/k3s-inventory/static.yaml.j2
```

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    {{ hostvars[inventory_hostname].iface }}:
      dhcp4: no
      addresses:
        - {{ hostvars[inventory_hostname].worker_ip }}/24
      routes:
        - to: default
          via: {{ ansible_default_ipv4.gateway }}
      nameservers:
        addresses:
{% for d in dns %}
          - {{ d }}
{% endfor %}
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-static-ip.yml
```

gen l·∫°i host
```bash
SUBNET=192.168.0.0/24
PORT=8022
USER="thang2k6adu"
START_IP=51
MASTER_IP=$(hostname -I | awk '{print $1}')
BASE_IP=$(echo $SUBNET | cut -d'/' -f1 | awk -F. '{print $1"."$2"."$3}')

mkdir -p ~/k3s-inventory && cd ~/k3s-inventory

echo -e "[master]\n$MASTER_IP ansible_user=$USER ansible_port=$PORT worker_ip=$MASTER_IP\n\n[workers]" > hosts.ini

sudo nmap -p $PORT --open $SUBNET \
| grep "Nmap scan report" \
| grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" \
| grep -v "^$MASTER_IP$" \
| awk -v USER="$USER" -v PORT="$PORT" -v BASE="$BASE_IP" -v START="$START_IP" \
'{print $0" ansible_user="USER" ansible_port="PORT" worker_ip="BASE"."START+NR-1}' \
>> hosts.ini

cd ~/

ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/gen_iface.yml -K
```

Check file inventory:
```bash
cat ~/k3s-inventory/hosts.ini
```

Check:
```bash
ansible workers -i ~/k3s-inventory/hosts.ini -m shell -a \
"echo '=== HOST:' \$(hostname) && ip a | grep inet && ip route | grep default && ping -c 2 8.8.8.8"
```

## M·ªû FIREWALL CHO WORKER (ANSIBLE)
```bash
nano ~/k3s-inventory/open-ufw-worker.yml
```
```yaml
- hosts: workers
  become: yes
  tasks:
    - name: Allow flannel VXLAN (8472/udp)
      ufw:
        rule: allow
        port: 8472
        proto: udp

    - name: Allow kubelet API (10250/tcp)
      ufw:
        rule: allow
        port: 10250
        proto: tcp

    - name: Enable UFW
      ufw:
        state: enabled
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/open-ufw-worker.yml
```

ƒë·ªïi t√™n node tr∆∞·ªõc khi join ƒë·ªÉ tr√°nh tr√πng t√™n

nano ~/k3s-inventory/set-hostname.yml

- hosts: workers
  become: yes
  gather_facts: yes

  tasks:
    - name: Set hostname based on last octet of IP
      hostname:
        name: "k3s-worker-{{ ansible_default_ipv4.address.split('.')[-1] }}"

    - name: Update /etc/hosts
      lineinfile:
        path: /etc/hosts
        regexp: "^{{ ansible_default_ipv4.address }}"
        line: "{{ ansible_default_ipv4.address }} k3s-worker-{{ ansible_default_ipv4.address.split('.')[-1] }}"
        state: present

    - name: Reboot to apply hostname
      reboot:
        reboot_timeout: 300

ch·∫°y
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/set-hostname.yml -K


## L·∫§Y TOKEN T·ª™ MASTER
```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

V√≠ d·ª•:
```
K10a3f9c8c7b2a3b7f9::server:xxxxxxxx
```

## C√ÄI K3S AGENT (WORKER)
```bash
nano ~/k3s-inventory/install-k3s-worker.yml
```
```yaml
- hosts: workers
  become: yes
  vars:
    k3s_url: "https://192.168.0.50:6443"
    k3s_token: "K10e6dd53c7c99770339ed79f4771c7ded0fbeee5baadfa6ed8224b56a80d5f43ce::server:78b31cbc69888b6ad8603eeb988b07a9"

  tasks:
    - name: Install k3s agent
      shell: |
        curl -sfL https://get.k3s.io | K3S_URL={{ k3s_url }} K3S_TOKEN={{ k3s_token }} sh -
```

Run:
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/install-k3s-worker.yml
```

Uninstall n·∫øu l·ªói:
```bash
nano ~/k3s-inventory/uninstall-k3s-worker.yml
```
```yaml
- hosts: workers
  become: yes

  tasks:
    - name: Stop k3s-agent service
      systemd:
        name: k3s-agent
        state: stopped
        enabled: false
      ignore_errors: yes

    - name: Run k3s-agent uninstall script
      shell: |
        if [ -f /usr/local/bin/k3s-agent-uninstall.sh ]; then
          /usr/local/bin/k3s-agent-uninstall.sh
        fi
      args:
        warn: false
      ignore_errors: yes

    - name: Remove k3s directories
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/rancher/k3s
        - /var/lib/rancher/k3s
        - /var/lib/kubelet
      ignore_errors: yes
```
```bash
ansible-playbook -i ~/k3s-inventory/hosts.ini ~/k3s-inventory/uninstall-k3s-worker.yml
```

## CHECK NODE ƒê√É JOIN
```bash
kubectl get nodes -o wide
```

Output:
```
NAME         STATUS   ROLES           IP
k3s-master   Ready    control-plane   192.168.0.50
worker1      Ready    <none>          192.168.0.505
worker2      Ready    <none>          192.168.0.506
```

## SET ROLE CHO WORKER
```bash
kubectl get nodes --no-headers | awk '{print $1}' | grep -v master | xargs -I {} kubectl label node {} node-role.kubernetes.io/worker=worker
```

Check:
```bash
kubectl get nodes
```

Output:
```
NAME            STATUS   ROLES    AGE
192.168.0.505   Ready    worker   1d
192.168.0.506   Ready    worker   1d
```

# üöÄ C√ÄI HELM + KUBERNETES DASHBOARD

## 1Ô∏è‚É£ C√†i Helm
```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version
```

b·∫Øt ƒë·∫ßu setup gitops
t·ª´ repo g·ªëc t·∫°o repo cluster-XXX (VD: Cluster Dev)

s·ª≠a c√°i ph·∫ßn ·ªü
- core component set
- tenants app set

sau ƒë√≥ c√†i ArgoCD tr∆∞·ªõc ƒë√£ (c√†i t·∫°m th√¥i)

kubectl create namespace argocd
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

sau ƒë√≥ bootstrap ArgoCD v√†o cluster
kubectl apply -k https://github.com/thang2k6adu/kubernetes-infra/cluster-dev/bootstrap/overlays/default

sau ƒë√≥ c√≥ th·ªÉ chuy·ªÉn n·ªôi dung c·ªßa install.yaml v√†o bootstrap.yaml

curl -L -o cluster-dev/bootstrap/base/install.yaml https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml


√ù nghƒ©a:
- t·∫°o namespace argocd
- c√†i Argo CD
- t·∫°o ApplicationSet
- Argo CD b·∫Øt ƒë·∫ßu t·ª± qu·∫£n l√Ω ch√≠nh n√≥
- deploy core + tenants

check

This should give you 4 applications

```shell
$ kubectl get applications -n argocd
NAME                          SYNC STATUS   HEALTH STATUS
bgd-blue                      Synced        Healthy
sample-admin-workload         Synced        Healthy
myapp                         Synced        Healthy
gitops-controller             Synced        Healthy
```

Backed by 2 applicationsets

```shell
$ kubectl get appsets -n argocd
NAME      AGE
cluster   110s
tenants   110s
```

ƒë·ªÉ xem argoCD UI, ƒë·∫ßu ti√™n c·∫ßn password

```shell
kubectl get secret/argocd-initial-admin-secret -n argocd -o jsonpath='{.data.password}' | base64 -d ; echo
```

sau ƒë·∫•y port forward (d√πng `admin` l√† user names)

```shell
kubectl -n argocd port-forward --address 0.0.0.0 service/argocd-server 8080:443
```

c√†i k8s dashboard

1 application trong gitops s·∫Ω nh∆∞ n√†y (k·ªÉ c·∫£ core hay tenants)

‚îú‚îÄ‚îÄ kustomization.yaml
‚îú‚îÄ‚îÄ namespace.yaml
‚îú‚îÄ‚îÄ deployment.yaml
‚îú‚îÄ‚îÄ service.yaml
‚îú‚îÄ‚îÄ ingress.yaml
‚îî‚îÄ‚îÄ configmap.yaml

quy t·∫Øc ƒë·∫∑t t√™n

<app-name>-<component>

VD
metadata:
  name: myapp-deployment
---
metadata:
  name: myapp-service
---
metadata:
  name: myapp-config
---
metadata:
  name: myapp

copy nguy√™n c√°i argo CD tr√™n trang ch√≠nh v·ªÅ t√°ch ra l√† xong

gi·ªù check

Check service:
```bash
kubectl get svc -n kubernetes-dashboard
```

l·∫•y token login
kubectl -n kubernetes-dashboard create token kubernetes-dashboard-admin

## 4Ô∏è‚É£ M·ªü proxy ƒë·ªÉ truy c·∫≠p Dashboard
```bash
sudo ufw allow 8001
kubectl proxy --address=0.0.0.0 --accept-hosts='^.*$'
```

N·∫øu kh√¥ng m·ªü proxy t·∫°i port `8001` th√¨ ph·∫£i v√†o `6443` (ch·∫Øc ch·∫Øn kh√¥ng v√†o ƒë∆∞·ª£c).


Truy c·∫≠p Dashboard:
```
http://192.168.0.50:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

Sau khi d√πng xong th√¨ ƒë√≥ng l·∫°i:
```bash
sudo ufw delete allow 8001
sudo ufw reload
```

setup ingress

disable traefik (n√†y l√†m th·ªß c√¥ng, ko gitops ƒë∆∞·ª£c v√¨ l√† server config)


Gh√©t traefik n√™n disable ƒëi:
```bash
sudo nano /etc/rancher/k3s/config.yaml
```

N·ªôi dung:
```yaml
disable:
  - traefik
```
```bash
sudo systemctl restart k3s
```

Check:
```bash
kubectl get pods -n kube-system
```

Config kube:
```bash
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $USER:$USER ~/.kube/config
```

fix l·ªói 127.0.0.1
echo 'export KUBECONFIG=/etc/rancher/k3s/k3s.yaml' >> ~/.bashrc
source ~/.bashrc

setup monitoring (c√°i d∆∞·ªõi ƒë√¢y ch·∫°y thay cho helm, t·ª´ l·∫ßn sau, c√°i n√†o m√† ch·∫°y helm th√¨ c·ª© application m√† gi√£)


th√™m c√°i configmap cho argoCD ƒë·ªÉ d√πng helm chart trong kustomization

Check:
```bash
kubectl get pods -n monitoring
```

Output:
```
prometheus-...
grafana-...
alertmanager-...
node-exporter-...
```

kubectl -n argocd port-forward --address 0.0.0.0 service/argocd-server 8080:443

ti·∫øp t·∫°o th√™m c√°i values cho ingress nginx



